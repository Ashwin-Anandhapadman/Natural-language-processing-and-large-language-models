{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "618b801d-90bf-435d-a06c-7e9dd8388d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c22231-2e2d-40c8-8338-add10302cdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashwi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf74d2bf-c112-4676-a3b7-e46d0046880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello! This is Ash's workspace. I am a man, a myth ! and lengend. I am an aspiring health AI researcher. \n",
    "This is my NLP project.\"\"\"\n",
    "\n",
    "corpus2= \"\"\"This is an apple, and that is an orange\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "387d0902-c8f5-46f0-99ab-7d41ed5e69e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! This is Ash's workspace. I am a man, a myth, and lengend. I am an aspiring health AI researcher. \\nThis is my NLP project.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67ae046d-e33c-4caf-9d89-d6b3ac0aef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! This is Ash's workspace. I am a man, a myth, and lengend. I am an aspiring health AI researcher. \n",
      "This is my NLP project.\n",
      "This is an apple, and that is an orange\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n",
    "\n",
    "print(corpus2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98029ecd-4b35-4e63-afee-c9829deba353",
   "metadata": {},
   "source": [
    "## #Tokenization: sentence creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "899a1f44-c5b1-4d3c-a187-1e5401bfb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c0c71b0-393c-4f65-9d00-55674642a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= sent_tokenize(corpus)\n",
    "#A sentence tokenizer, like the one provided by the punkt resource in NLTK, is a tool that segments a string of text into individual sentences.\n",
    "#it splits the sentece wherever there is \".\" and \"!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff896796",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2= sent_tokenize(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc42042-5b13-4901-b58d-eda6bc15e54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65794e98-d768-4976-b7cd-8efd82f3cc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "This is Ash's workspace.\n",
      "I am a man, a myth !\n",
      "and lengend.\n",
      "I am an aspiring health AI researcher.\n",
      "This is my NLP project.\n"
     ]
    }
   ],
   "source": [
    "for sent in docs:\n",
    "    print(sent)\n",
    "    #new token after ! and ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fba398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an apple, and that is an orange\n"
     ]
    }
   ],
   "source": [
    "for sent in docs2:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6895d4-5dcb-4658-be56-ae43e2abe7f6",
   "metadata": {},
   "source": [
    "## Tokenization2: Paragraph to words and Sentence to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1d86eef-c9ab-4a37-8cbf-19c5456c54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4311e2b8-13f8-4d16-af3b-286e7db7d1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'Ash',\n",
       " \"'s\",\n",
       " 'workspace',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'man',\n",
       " ',',\n",
       " 'a',\n",
       " 'myth',\n",
       " '!',\n",
       " 'and',\n",
       " 'lengend',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'an',\n",
       " 'aspiring',\n",
       " 'health',\n",
       " 'AI',\n",
       " 'researcher',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'NLP',\n",
       " 'project',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ef7d172-49ec-4695-8585-e8a9578828fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!']\n",
      "['This', 'is', 'Ash', \"'s\", 'workspace', '.']\n",
      "['I', 'am', 'an', 'aspiring', 'health', 'AI', 'researcher', '.']\n",
      "['This', 'is', 'my', 'NLP', 'project', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in docs:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bcf586d2-b3b1-41c0-8757-c1f210330878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dea055e6-1bde-4736-b384-787de4b7dd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'Ash',\n",
       " \"'\",\n",
       " 's',\n",
       " 'workspace',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'an',\n",
       " 'aspiring',\n",
       " 'health',\n",
       " 'AI',\n",
       " 'researcher',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'NLP',\n",
       " 'project',\n",
       " '.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus) #here the \" 's\" in \"Ash's\" will be split too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3544ac7b-a3e3-49ad-8491-e10ee9a93aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b28a79e5-d3f3-4f5c-b59d-fac7ce28957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b6726467-7c94-4cba-87e1-2d629404ce02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'Ash',\n",
       " \"'s\",\n",
       " 'workspace.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'an',\n",
       " 'aspiring',\n",
       " 'health',\n",
       " 'AI',\n",
       " 'researcher.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'NLP',\n",
       " 'project',\n",
       " '.']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus) #the fullstops in between sentences are included within a word itself. last full stop alone is a seperate token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007cd330-8726-4d16-a7f0-95ec56d89c1c",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6d15260f-7139-4c77-8d4c-ac605a9811ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\", \"eaten\", \"eats\", \"writing\", \"history\",\"write\", \"programming\", \"programs\", \"goes\", \"going\", \"gone\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b436ff-b9b1-47f3-8dd7-2c133867c704",
   "metadata": {},
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "19ce94b6-7a21-4857-9a2f-7cbb4ea0092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5742466b-96ce-4a11-970b-b0471105bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem1= PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ff24a2c5-5891-48ac-9ae9-8789eade4f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eaten----->eaten\n",
      "eats----->eat\n",
      "writing----->write\n",
      "history----->histori\n",
      "write----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "goes----->goe\n",
      "going----->go\n",
      "gone----->gone\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"----->\"+stem1.stem(word))\n",
    "\n",
    "#as we see below some words like history are totally different now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1046597f-6ae3-463d-8d7f-629a8788dcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "congratul\n",
      "sit\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "print(stem1.stem('congratulation'))\n",
    "print(stem.stem('sitting'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52575605-e241-458a-8cd4-1d16e7441107",
   "metadata": {},
   "source": [
    "### RegexpStemmer (for regular expression stemmer algoirthms)\n",
    "\n",
    "it takes a regular experession and removes any prefix/suffix that matches the exp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11b8854d-f509-4cb1-a5dc-bedaebe4ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f814c5a-8f8a-4846-91b6-f149e1aea190",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stem= RegexpStemmer('ing$|s$|e$|able$', min=4) #whatever matches the exp is removed in prefix or suffix\n",
    "\n",
    "#min=4 means the word should be atleast 4 characters long.  if not it will not be stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be1cfa-24e0-459a-980c-215f9460eb0e",
   "metadata": {},
   "source": [
    "The min parameter ensures that only words meeting a certain length are processed, which helps avoid altering short words unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c900190-e9cc-421f-b06c-1ba8068a8896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wash'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem('washing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2e173a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ing'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem('ing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aeebd3a2-162e-4e80-997a-ebccf5afd023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingwash'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem('ingwashing') \n",
    "#to be able to remove the first ing- we need to remove the $ sign in the regular exp defintion at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "95791bea-1967-42ca-a2f0-095fdb0b1984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wash'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem2= RegexpStemmer('ing|s$|e$|able$', min=4)\n",
    "reg_stem2.stem('ingwashing') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f04ca-3bd6-4599-a5de-62640ded29dc",
   "metadata": {},
   "source": [
    "### SnowBall stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "10766a8e-183d-4f54-9ac7-a0e425443af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "27da8525-850b-4326-b9a9-dc70eaa14df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_stem= SnowballStemmer('english')\n",
    "\n",
    "#Snowball stem supports many languages like: Arabic, Danish, Dutch, English etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ad9d4b4-2d5f-4fec-a1ae-a9b4336b2209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eaten----->eaten\n",
      "eats----->eat\n",
      "writing----->write\n",
      "history----->histori\n",
      "write----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "goes----->goe\n",
      "going----->go\n",
      "gone----->gone\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"----->\"+snow_stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "923174e9-838f-4689-9d9b-3c6073b9d95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairli sprotingli\n",
      "fair sport\n"
     ]
    }
   ],
   "source": [
    "#compare btw porter and snowball stemmers\n",
    "\n",
    "print(stem1.stem(\"fairly\"), stem1.stem(\"sprotingly\"))\n",
    "print(snow_stem.stem(\"fairly\"), snow_stem.stem(\"sportingly\"))\n",
    "\n",
    "#snowball stem is slightly better than porter stemmer in terms of capturing the meaning. Still some\n",
    "# words like history are not stemmed well by snow ball too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc1c8f-419e-46f9-a3ae-364e62b9b219",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a8340-1061-4732-943f-1772237f9c85",
   "metadata": {},
   "source": [
    "lemmatization tries to solve the issues facing the above stemmers in terms of not \n",
    "chaning the meaning of the word entirely like the porter or snow ball. \n",
    "\n",
    "In the lemmatizers we get the root word rather than root stem (like what we got with the stems above). It uses a kind of reference dictionary to create the roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ffebe961-b4bf-4071-87b4-d2fbf8fd49c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashwi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Wordnet Lemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9c133cef-96c4-4520-89c5-366042bb3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3d4b4c6c-31f8-436b-a079-5eb3e7d6c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "23f741eb-b24b-4071-b044-9c9fdcec8b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "print(lemm.lemmatize(\"going\",pos='n'))\n",
    "print(lemm.lemmatize(\"going\",pos='v'))\n",
    "\n",
    "#the pos attribute signals wjether the root should be based on a noun(n), vern(v). adjective (a) etc\n",
    "#default is noun(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0c6a455d-abe2-477e-b626-54dfa91e43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2=[\"eating\", \"eaten\", \"eats\", \"writing\", \"history\",\"write\",\"goes\", \"fairly\", \"programming\", \"programs\", \"goes\", \"going\", \"gone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7e25a3b4-0653-4f7d-b2df-856648bba023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eating\n",
      "eaten----->eaten\n",
      "eats----->eats\n",
      "writing----->writing\n",
      "history----->history\n",
      "write----->write\n",
      "goes----->go\n",
      "fairly----->fairly\n",
      "programming----->programming\n",
      "programs----->program\n",
      "goes----->go\n",
      "going----->going\n",
      "gone----->gone\n"
     ]
    }
   ],
   "source": [
    "for word in words2:\n",
    "    print(word+\"----->\"+lemm.lemmatize(word, pos='n'))\n",
    "\n",
    "#now words like history is perfect because nouns are not distrubed here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fe6353ed-9aa2-43bd-8cb9-4ac1639268aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eaten----->eat\n",
      "eats----->eat\n",
      "writing----->write\n",
      "history----->history\n",
      "write----->write\n",
      "goes----->go\n",
      "fairly----->fairly\n",
      "programming----->program\n",
      "programs----->program\n",
      "goes----->go\n",
      "going----->go\n",
      "gone----->go\n"
     ]
    }
   ],
   "source": [
    "#trying the verb format\n",
    "\n",
    "for word in words2:\n",
    "    print(word+\"----->\"+lemm.lemmatize(word, pos='v'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb9b7f-f6a4-4714-81cc-707f4c94418a",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e4fad4-6535-407b-b648-51915effe69b",
   "metadata": {},
   "source": [
    "Stop words are commonly used words in a language that are often filtered out during NLP\n",
    "tasks because they carry little meaningful information. Examples include \"a,\" \"the,\" \"is,\" and \"are.\" The primary purpose of removing stop words is to focus on more significant words that contribute to the overall understanding of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5a630fec-33ad-4739-beac-bd9d43f781a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d77e1d5b-c50f-462d-8ca9-e66a5bd494c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashwi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cae63ce3-b5d6-42ad-b058-cbf9a8504910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('ENGLISH') #these words will be removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e8db0168-d9b0-46b6-8c14-16c553915165",
   "metadata": {},
   "outputs": [],
   "source": [
    "para= \"\"\" Cricket is one of the most celebrated sports globally, with a rich history that has captivated millions of fans. Originating in England, the game has evolved into a major international sport, governed by the International Cricket Council (ICC). The format of cricket varies from Test matches, which can last up to five days, to One Day Internationals (ODIs) and the fast-paced Twenty20 (T20) games. This diversity in formats allows for different styles of play and strategies, making cricket an exciting spectacle. Legendary players like Sachin Tendulkar, Sir Don Bradman, and Shane Warne have left an indelible mark on the sport, showcasing exceptional talent and sportsmanship that continue to inspire new generations.\n",
    "\n",
    "The fame of cricket is not just limited to its players; it also encompasses iconic tournaments such as the ICC Cricket World Cup and domestic leagues like the Indian Premier League (IPL). These events draw massive audiences and generate significant revenue, further solidifying cricket's status as a global phenomenon. Countries like India, Australia, and England have passionate fan bases that contribute to the sport's vibrant culture. The emotional connection fans have with their teams and players often transcends geographical boundaries, creating a unique sense of community among cricket lovers worldwide. As cricket continues to grow in popularity, it remains a powerful symbol of unity and competition across diverse cultures.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "efd33b9b-ad86-403b-893c-c1f6f2588b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences= nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "55dc92b8-9203-4431-93bf-842a2cc8bb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Cricket is one of the most celebrated sports globally, with a rich history that has captivated millions of fans.',\n",
       " 'Originating in England, the game has evolved into a major international sport, governed by the International Cricket Council (ICC).',\n",
       " 'The format of cricket varies from Test matches, which can last up to five days, to One Day Internationals (ODIs) and the fast-paced Twenty20 (T20) games.',\n",
       " 'This diversity in formats allows for different styles of play and strategies, making cricket an exciting spectacle.',\n",
       " 'Legendary players like Sachin Tendulkar, Sir Don Bradman, and Shane Warne have left an indelible mark on the sport, showcasing exceptional talent and sportsmanship that continue to inspire new generations.',\n",
       " 'The fame of cricket is not just limited to its players; it also encompasses iconic tournaments such as the ICC Cricket World Cup and domestic leagues like the Indian Premier League (IPL).',\n",
       " \"These events draw massive audiences and generate significant revenue, further solidifying cricket's status as a global phenomenon.\",\n",
       " \"Countries like India, Australia, and England have passionate fan bases that contribute to the sport's vibrant culture.\",\n",
       " 'The emotional connection fans have with their teams and players often transcends geographical boundaries, creating a unique sense of community among cricket lovers worldwide.',\n",
       " 'As cricket continues to grow in popularity, it remains a powerful symbol of unity and competition across diverse cultures.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c2c01611-f6f7-4831-aa9d-433a70dc6c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b7040ff0-14dc-4dfe-a96d-6a53b9dcda5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply stop words and filter...then apply stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words3 = nltk.word_tokenize(sentences[i])\n",
    "    words3 = [stem1.stem(word) for word in words3 if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words3)\n",
    "\n",
    "#stem1 is for the port stemmer we intialized before. we do the stemming for the words not present in the stopwords dictionary\n",
    "#likewise we can apply snowball stemmer on the senetences too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6515949c-8242-45f4-a609-ed6a680bdd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cricket one celebr sport global , rich histori captiv million fan .',\n",
       " 'origin england , game evolv major intern sport , govern intern cricket council ( icc ) .',\n",
       " 'format cricket vari test match , last five day , one day intern ( odi ) fast-pac twenty20 ( t20 ) game .',\n",
       " 'thi diver format allow differ style play strategi , make cricket excit spectacl .',\n",
       " 'legendari player like sachin tendulkar , sir bradman , shane warn left indel mark sport , showca except talent sportsmanship continu inspir new gener .',\n",
       " 'fame cricket limit player ; also encompass icon tournament icc cricket world cup domest leagu like indian premier leagu ( ipl ) .',\n",
       " \"event draw massiv audienc gener signif revenu , solidifi cricket 's statu global phenomenon .\",\n",
       " \"countri like india , australia , england passion fan base contribut sport 's vibrant cultur .\",\n",
       " 'emot connect fan team player often transcend geograph boundari , creat uniqu sen commun among cricket lover worldwid .',\n",
       " 'cricket continu grow popular , remain power symbol uniti competit across diver cultur .']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f76ed-9edb-480e-84f8-f37ca2a629a6",
   "metadata": {},
   "source": [
    "# Lemmatizer with stopwords\n",
    "\n",
    "Can filter words based on verb, adverb and noun etc. Helps maintain the meaning of the word very well. The root word is called lemma. It uses wordnet corpus as a reference for getting the root words. This is time-consuming process when applied on big corpus, relative to stemming (even though results are better than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c86ad93f-4aa3-42d1-be32-da470d1b7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma2= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b8e8446d-0d98-4e55-bbb6-aaeb6594e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply stop words and filter...then apply stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words4 = nltk.word_tokenize(sentences[i])\n",
    "    words4 = [lemma2.lemmatize(word, pos='v') for word in words4 if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words4)\n",
    "\n",
    "#stem1 is for the port stemmer we intialized before. we do the stemming for the words not present in the stopwords dictionary\n",
    "#likewise we can apply snowball stemmer on the senetences too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8f39841d-2b1b-4f7a-88d3-3ca769ce0c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cricket one celebr sport global , rich histori captiv million fan .',\n",
       " 'origin england , game evolv major intern sport , govern intern cricket council ( icc ) .',\n",
       " 'format cricket vari test match , last five day , one day intern ( odi ) fast-pac twenty20 ( t20 ) game .',\n",
       " 'thi diver format allow differ style play strategi , make cricket excit spectacl .',\n",
       " 'legendari player like sachin tendulkar , sir bradman , shane warn leave indel mark sport , showca except talent sportsmanship continu inspir new gener .',\n",
       " 'fame cricket limit player ; also encompass icon tournament icc cricket world cup domest leagu like indian premier leagu ( ipl ) .',\n",
       " \"event draw massiv audienc gener signif revenu , solidifi cricket 's statu global phenomenon .\",\n",
       " \"countri like india , australia , england passion fan base contribut sport 's vibrant cultur .\",\n",
       " 'emot connect fan team player often transcend geograph boundari , creat uniqu sen commun among cricket lover worldwid .',\n",
       " 'cricket continu grow popular , remain power symbol uniti competit across diver cultur .']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990fc4e2-e7f2-437c-b162-2ac75ba427e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29089b8d-80e7-49ee-ba6c-f7ee4e4379c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592bac26-f5c3-4adf-9912-38448c9a78f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c2b8c-1fca-4faf-981a-fdc4497f6505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c33015-4cb0-4659-8016-8c115f8bb2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67260444-b31a-4dc9-9830-459fde4e8da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f729f328-f976-40fe-b30f-38dc8a592522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f1394-ea79-451e-9256-f633612c559c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69ec60-eb67-471c-b581-74a813ff601d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d07b5-3d37-4ac3-9a13-fd75e3aaa996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8daf9-de37-43d5-b3f5-b39b83ad82f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e75347-5905-4afb-9a6d-340c5f3b6219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888fec9-3b17-408f-ab5b-cfcbef5062df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd68180-c0da-44db-98e7-ead0085635ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be6ce4-16ab-4384-ad83-d8a35b5a26d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
